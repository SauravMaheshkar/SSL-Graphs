{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwL-69lbX4yM"
   },
   "source": [
    "# [**Deep Graph Contrastive Representation Learning**](https://arxiv.org/abs/2006.04131v2)\n",
    "\n",
    "By Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu and Liang Wang\n",
    "\n",
    "[Original Implementation](https://github.com/CRIPAC-DIG/GRACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klxDavaKxn3K"
   },
   "source": [
    "# ðŸ“¦ Packages and Basic Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "px5bFLDExLOZ",
    "outputId": "208ae658-e661-4e86-c503-7ccf5da0ce76"
   },
   "outputs": [],
   "source": [
    "# Install correct versions of PyTorch Geometric packages\n",
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "\n",
    "!pip install torch-scatter -qq -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
    "!pip install torch-sparse -qq -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
    "!pip install torch-cluster -qq -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
    "!pip install torch-spline-conv -qq -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
    "!pip install -qq torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hny-Z_vgyDxG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import perf_counter as t\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from torch_geometric.datasets import Planetoid, CitationFull\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYWLr5QDX150"
   },
   "source": [
    "### âš™ Hyperparameters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzwITD670Ns-"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "num_hidden = 128\n",
    "num_proj_hidden = 128\n",
    "activation = F.relu\n",
    "base_model = GCNConv\n",
    "num_layers = 2\n",
    "drop_edge_rate_1 = 0.2\n",
    "drop_edge_rate_2 = 0.4\n",
    "drop_feature_rate_1 = 0.3\n",
    "drop_feature_rate_2 = 0.4\n",
    "tau = 0.4\n",
    "num_epochs = 200\n",
    "weight_decay = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPFaqh1OzrHd"
   },
   "source": [
    "# ðŸ’¿ Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8-_pUeO9ZwO"
   },
   "source": [
    "**Reported** Performance on Transductive Tasks for various datasets\n",
    "\n",
    "| Method | Training Data | Cora     | Citeseer | Pubmed   | DBLP     |\n",
    "|--------|---------------|----------|----------|----------|----------|\n",
    "| GRACE  | X, A          | 83.3Â±0.4 | 72.1Â±0.5 | 86.7Â±0.1 | 84.2Â±0.1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGGW_eoBzvyr"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "\n",
    "def get_dataset(path: str, name: str = \"Cora\") -> InMemoryDataset:\n",
    "\n",
    "    # Only Supports Cora, CiteSeer, PubMed and DBLP for now\n",
    "    supported_datasets = [\"Cora\", \"CiteSeer\", \"PubMed\", \"DBLP\"]\n",
    "    assert name in supported_datasets, \"Invalid Dataset\"\n",
    "\n",
    "    return (CitationFull if name == \"DBLP\" else Planetoid)(\n",
    "        root=path, name=name, transform=T.NormalizeFeatures()\n",
    "    )\n",
    "\n",
    "\n",
    "# Download Dataset\n",
    "path = os.path.join(os.path.expanduser(\"~\"), \"datasets\", \"Cora\")\n",
    "dataset = get_dataset(path, \"Cora\")\n",
    "data = dataset[0]\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5iG8UaaxxmU"
   },
   "source": [
    "# âœï¸ Model Architecture & Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9ER9AuDx4Oj"
   },
   "source": [
    "## Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huY_IXp9Ogcs"
   },
   "source": [
    "In the following cells, we implement the Encoder used to generate views and the Model we'll use. The two points of focus here are the two components of the overall loss objective ðŸ“‰:-\n",
    "\n",
    "1. Pairwise Objective for each positive pair $(u_i, v_i)$: (implemented in `semi_loss` and `batched_semi_loss`)\n",
    "\n",
    "$$\n",
    "\\large\n",
    "l(u_i, v_i) = \\text{log} \\frac{e^{\\theta(u_i, v_i)/ \\tau}}{{e^{\\theta(u_i, v_i)/ \\tau}} + \\color{yellow}{\\sum_{k=1}^{N}\\mathbb{1}_{[k \\neq 1]} e^{\\theta(u_i, v_k)/ \\tau}} + \\color{green}{\\sum_{k=1}^{N}\\mathbb{1}_{[k \\neq 1]} e^{\\theta(u_i, u_k)/ \\tau}}}\n",
    "$$\n",
    "\n",
    "2. Joint Training Objective $J$: (implemented in `loss`)\n",
    "\n",
    "$$\n",
    "\\large\n",
    "J = \\frac{1}{2N} \\sum_{i=1}^{N} \\left[ l(u_i, v_i) + l(v_i, u_i) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVac7UMrx5Rf"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    For more details refer to Section 4.2 of the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation=F.relu,\n",
    "        base_model=GCNConv,\n",
    "        k: int = 2,\n",
    "    ) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        # Build the Model\n",
    "        assert k >= 2, f\"k needs to be atleast 2\"\n",
    "        self.k = k\n",
    "        self.conv = [base_model(in_channels, 2 * out_channels)]\n",
    "        for _ in range(1, k - 1):\n",
    "            self.conv.append(base_model(2 * out_channels, 2 * out_channels))\n",
    "        self.conv.append(base_model(2 * out_channels, out_channels))\n",
    "        self.conv = nn.ModuleList(self.conv)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute a forward pass\"\"\"\n",
    "        for i in range(self.k):\n",
    "            x = self.activation(self.conv[i](x, edge_index))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: torch.nn.Module,\n",
    "        num_hidden: int,\n",
    "        num_proj_hidden: int,\n",
    "        tau: float = 0.5,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.tau = tau\n",
    "\n",
    "        # To generate the projections, we use a two-layer MLP\n",
    "        # For details, refer to section 3.2.1\n",
    "        self.fc1 = torch.nn.Linear(num_hidden, num_proj_hidden)\n",
    "        self.fc2 = torch.nn.Linear(num_proj_hidden, num_hidden)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute a forward pass through the encoder\"\"\"\n",
    "        return self.encoder(x, edge_index)\n",
    "\n",
    "    def projection(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the projections for a given view,\n",
    "        shown to enhance the expression power of the critic,\n",
    "        For details, refer to Section 3.2.1\n",
    "        \"\"\"\n",
    "        z = F.elu(self.fc1(z))\n",
    "        return self.fc2(z)\n",
    "\n",
    "    def cosine_sim(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the Cosine Similarity between two given views\n",
    "        \"\"\"\n",
    "        z1 = F.normalize(z1)\n",
    "        z2 = F.normalize(z2)\n",
    "        return torch.mm(z1, z2.t())\n",
    "\n",
    "    def semi_loss(self, z1: torch.Tensor, z2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the 'semi loss' between two given views\n",
    "        \"\"\"\n",
    "        f = lambda x: torch.exp(x / self.tau)\n",
    "        intraview_pairs = f(self.cosine_sim(z1, z1))\n",
    "        interview_pairs = f(self.cosine_sim(z1, z2))\n",
    "\n",
    "        return -torch.log(\n",
    "            interview_pairs.diag()\n",
    "            / (intraview_pairs.sum(1) + interview_pairs.sum(1) - intraview_pairs.diag())\n",
    "        )\n",
    "\n",
    "    def batched_semi_loss(\n",
    "        self, z1: torch.Tensor, z2: torch.Tensor, batch_size: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the `semi loss` between a batch of two given views\n",
    "        Space complexity: O(BN) (semi_loss: O(N^2))\n",
    "        \"\"\"\n",
    "        device = z1.device\n",
    "        num_nodes = z1.size(0)\n",
    "        num_batches = (num_nodes - 1) // batch_size + 1\n",
    "        f = lambda x: torch.exp(x / self.tau)\n",
    "        indices = torch.arange(0, num_nodes).to(device)\n",
    "        losses = []\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            # Mask out other values not in the current batch\n",
    "            mask = indices[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            # Similar to self.semi_loss()\n",
    "            intraview_pairs = f(self.cosine_sim(z1[mask], z1))  # [B, N]\n",
    "            interview_pairs = f(self.cosine_sim(z1[mask], z2))  # [B, N]\n",
    "\n",
    "            losses.append(\n",
    "                -torch.log(\n",
    "                    interview_pairs[:, i * batch_size : (i + 1) * batch_size].diag()\n",
    "                    / (\n",
    "                        intraview_pairs.sum(1)\n",
    "                        + interview_pairs.sum(1)\n",
    "                        - intraview_pairs[\n",
    "                            :, i * batch_size : (i + 1) * batch_size\n",
    "                        ].diag()\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return torch.cat(losses)\n",
    "\n",
    "    def loss(\n",
    "        self, z1: torch.Tensor, z2: torch.Tensor, mean: bool = True, batch_size: int = 0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Overall objective for all positive pairs\n",
    "        Eqn (2) from the paper\n",
    "        \"\"\"\n",
    "\n",
    "        # ---- Generate views\n",
    "        # one is used as the anchor\n",
    "        # the other forms the positive sample\n",
    "        u = self.projection(z1)\n",
    "        v = self.projection(z2)\n",
    "\n",
    "        # As the two views are symmetric\n",
    "        # the other loss is just calculated\n",
    "        # using alternate parameters\n",
    "\n",
    "        if batch_size == 0:\n",
    "            l1 = self.semi_loss(u, v)\n",
    "            l2 = self.semi_loss(v, u)\n",
    "        else:\n",
    "            l1 = self.batched_semi_loss(u, v, batch_size)\n",
    "            l2 = self.batched_semi_loss(v, u, batch_size)\n",
    "\n",
    "        ret = (l1 + l2) * 0.5\n",
    "        ret = ret.mean() if mean else ret.sum()\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-hhX33fBISJ"
   },
   "outputs": [],
   "source": [
    "# Instantiate the Encoder and the Model\n",
    "encoder = Encoder(\n",
    "    in_channels=dataset.num_features,\n",
    "    out_channels=num_hidden,\n",
    "    activation=activation,\n",
    "    base_model=base_model,\n",
    "    k=num_layers,\n",
    ").to(device)\n",
    "\n",
    "model = Model(encoder, num_hidden, num_proj_hidden, tau).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZlHT--Fx2KQ"
   },
   "source": [
    "## â›‘ï¸ Utility Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRnURMmjxszk"
   },
   "outputs": [],
   "source": [
    "def drop_feature(x: torch.Tensor, drop_prob: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implemention of Masking Node Features (MF) randomly masks\n",
    "    a fraction of dimensions with zeros in node features\n",
    "    \"\"\"\n",
    "    drop_mask = (\n",
    "        torch.empty((x.size(1),), dtype=torch.float32, device=x.device).uniform_(0, 1)\n",
    "        < drop_prob\n",
    "    )\n",
    "    x = x.clone()\n",
    "    x[:, drop_mask] = 0\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def train_step(model: torch.nn.Module, x: torch.Tensor, edge_index: torch.Tensor):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Graph View Generation\n",
    "\n",
    "    ## Removing Edges (RE)\n",
    "    edge_index_1 = dropout_adj(edge_index, p=drop_edge_rate_1)[0]\n",
    "    edge_index_2 = dropout_adj(edge_index, p=drop_edge_rate_2)[0]\n",
    "    ## Masking Node Features (MF)\n",
    "    x_1 = drop_feature(x, drop_feature_rate_1)\n",
    "    x_2 = drop_feature(x, drop_feature_rate_2)\n",
    "    ## Generating views\n",
    "    z1 = model(x_1, edge_index_1)\n",
    "    z2 = model(x_2, edge_index_2)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = model.loss(z1, z2, batch_size=0)\n",
    "\n",
    "    # Perform Backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU5uPES2yaTS"
   },
   "source": [
    "## Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-rQcgbzyZty",
    "outputId": "fc7f3dee-428f-4633-fde7-0959f564ed7a"
   },
   "outputs": [],
   "source": [
    "# create a optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Set the model into Training Mode\n",
    "model.train()\n",
    "\n",
    "# Run training\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_step(model, data.x, data.edge_index)\n",
    "\n",
    "    print(f\"Epoch={epoch:03d} | loss={loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
